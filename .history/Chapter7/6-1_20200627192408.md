

<!--
 * @version:
 * @Author: steven
 * @Date: 2020-06-27 19:08:10
 * @LastEditors: steven
 * @LastEditTime: 2020-06-27 19:24:08
 * @Description:
-->
# 6.1. 从密集的层到卷积

当我们处理表格数据时，我们到目前为止所讨论的模型仍然是(直到今天)合适的选择。所谓表格，是指数据由与示例对应的行和与特征对应的列组成。使用表格数据，我们可能会预期我们寻找的模式可能涉及到特征之间的交互，但是我们不会假设任何关于特征如何交互的先验结构。

有时候，我们确实缺乏知识来指导工匠建筑的建造。在这些情况下，多层感知器可能是我们能做的最好的。然而，对于高维感知数据，这些无结构网络可能会变得笨拙。

例如，让我们回到我们的例子，区分猫和狗。假设我们在数据收集方面做了彻底的工作，收集了一个带有注解的100万像素照片数据集。这意味着网络的每个输入都有100万个维度。即使是将隐藏维度减少到1000，也需要一个由109109个参数组成的稠密(完全连接)层。除非我们有大量的gpu、分布式优化的天赋和非凡的耐心，学习这个网络的参数可能是不可行的。

细心的读者可能会反对这个论点，因为100万像素的分辨率可能没有必要。然而，尽管我们可能能够获得100,000像素，但我们10001000的隐藏层严重低估了学习图像良好表示所需的隐藏节点的数量，因此一个实际的系统仍然需要数十亿个参数。此外，通过拟合这么多参数来学习分类器可能需要收集一个庞大的数据集。然而今天，人类和电脑都能很好地区分猫和狗，这似乎有些矛盾。

## 6.1.1. 不变性

假设您想要检测图像中的一个对象。无论我们使用什么方法来识别物体，都不应该过分关注物体在图像中的精确位置，这似乎是合理的。理想情况下，我们的系统应该利用这些知识。猪通常不会飞，飞机通常不会游泳。尽管如此，我们还是应该能认出出现在图像顶部的是一头猪。我们可以从儿童游戏中得到一些启发，其中s Waldo(如图6.1.1所示)。

现在，我们可以通过列举一些需求来指导我们设计适合计算机视觉的神经网络结构，从而使这些直觉更加具体:在最早的层中，我们的网络应该对同一个补丁做出类似的响应，而不管它出现在图像的哪个位置(平移不变量)。最早的网络层应该关注局部区域，而不考虑遥远区域(locality)的图像内容。最终，这些局部表示可以聚合在整个图像级别进行预测。

## 6.1.2. 约束MLP

TODO:

现在让我们引用上面建立的第一个原则:平移不变性。这意味着输入xx的位移会导致激活hh的位移。这只有在V和u实际上不依赖于(i,j)时才有可能。V[i,j,a,b]=V[a,b]， u是常数。因此，我们可以简化h的定义。

在进一步讨论之前，我们应该简要回顾一下为什么上述操作被称为卷积。在数学中，两个函数之间的卷积，比如f,g:Rd Rf,g:Rd R被定义为

即测量一个函数翻转移位xx时ff与gg之间的重叠。当我们有离散的对象时，积分就变成了和。例如，对于定义在22上的向量，即。索引运行在ZZ上的可平方和无限维向量的集合，我们得到以下定义。

对于二维数组，ff对应索引(i,j)， gg对应索引(i a,j b)。这看起来类似于上面的定义，但有一个主要的区别。我们不是使用(i+a,j+b)(i+a,j+b)，而是使用差值。但是请注意，这种区别主要是修饰性的，因为我们总是可以使用V~[a,b]=V[a,b]来匹配符号，从而得到h=x V~。我们最初的定义更恰当地描述了相互关系。我们将在下一节中回到这个问题。

## 6.1.4. 重新审视Waldo检测器，
回到我们的Waldo检测器，让我们看看这是什么样子的。卷积层选择给定大小的窗口，并根据掩模V来衡量强度，如图6.1.2所示。我们可以学习一种模型，以便在waldoness最高的地方，我们应该在隐藏层激活中找到一个峰值。

Fig. 6.1.2 Find Waldo.

这种方法只有一个问题。到目前为止，我们幸运地忽略了图像由3个通道组成:红、绿、蓝。实际上，图像不是二维物体，而是3阶张量，其特征是高度、宽度和通道，例如形状为1024 1024 31024 1024 3像素。这些轴的前两个涉及空间关系，而第3rd3可以被视为为每个像素位置分配了一个多维表示。

因此，我们将x索引为x[i,j,k]。卷积掩码必须相应地进行调整。我们现在有了V[a,b,c]。

而且，就像我们的输入是由一个3rd3阶张量组成的一样，用类似的方式来表示我们隐藏的表示，也不失为一个好主意。换句话说，与每个空间位置对应的单个激活不同，我们需要与每个空间位置对应的隐藏激活的整个向量。我们可以把隐藏的表示法看作是由若干个相互堆叠的2D网格组成的。在输入中，这些有时被称为通道。它们有时也被称为特征图，因为每一个都提供了一个空间化的集合

这就是卷积神经网络层的定义。我们仍有许多业务需要处理。例如，我们需要弄清楚如何将所有激活组合为单个输出(例如，在映像中的任何位置是否有Waldo)。我们还需要决定如何高效地计算，如何组合多层，适当的激活函数，以及如何做出合理的设计选择，以产生实际有效的网络。我们将在本章的其余部分讨论这些问题。

## 6.1.5. 总结

图像中的平移不变性意味着图像中的所有小块都将以相同的方式处理。局部性是指只使用像素的一个小邻域来计算相应的隐藏激活。输入和输出通道允许我们的模型在每个空间位置捕获图像的多个方面。

## 6.1.6. 练习

1. 假设卷积面具的大小是Δ= 0。说明在这种情况下，卷积掩码独立地为每组通道实现了一个MLP。
2. 为什么平移不变性不是一个好主意呢?什么时候允许猪飞是没有意义的呢?在决定如何处理与图像边界像素位置相对应的激活时，我们必须处理哪些问题?描述一个类似的音频卷积层。您认为卷积层也适用于文本数据吗?为什么或为什么不?证明f g=g f
3. 在决定如何处理与图像边界像素位置相对应的激活时，我们必须处理哪些问题
4. 描述一个类似的音频卷积层。
5. 您认为卷积层也适用于文本数据吗?为什么或为什么不
6. 证明f⊛g=g⊛f


