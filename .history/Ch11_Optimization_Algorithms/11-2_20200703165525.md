

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-03 16:25:07
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-03 16:55:25
 * @Description:
 * @TODO::
 * @Reference:
-->

# 凸性

凸性在优化算法设计中起着至关重要的作用。这很大程度上是因为在这种环境下分析和测试算法要容易得多。换句话说，如果算法即使在凸集上也表现不佳，我们就不应该希望看到好的结果。此外，尽管深度学习中的优化问题通常是非凸问题，但它们往往在局部极小点附近表现出凸问题的一些性质。这可能导致令人兴奋的新的优化变体，如 [Izmailov et al., 2018](http://preview.d2l.ai/d2l-en/PR-1102/chapter_references/zreferences.html#izmailov-podoprikhin-garipov-ea-2018)。

## 基础

让我们从基础开始。

这听起来有点抽象。考虑[图11.2.1](http://preview.d2l.ai/d2l-en/PR-1102/chapter_optimization/convexity.html#fig-pacman)所示的图片。第一个集合不是凸的，因为它不包含线段。另外两组则没有这样的问题。


为了说明这一点，让我们绘制一些函数，并检查哪些满足需求。我们需要导入一些库。

TODO:CODE

让我们定义几个函数，凸函数和非凸函数。

TODO:CODE

正如预期的那样，余弦函数是非凸的，而抛物线和指数函数是非凸的。注意，要使条件有意义，X 是凸集这一要求是必要的。否则，f(λx+(1−λ)x′) 的结果可能没有很好的定义。凸函数有许多令人满意的性质。

## Jensen不等式

Jensen不等式是最有用的工具之一。它相当于凸性定义的一般化

其中， α_i 是非负的实数，例如 ∑iαi=1  。换句话说，凸函数的期望比期望的凸函数要大。为了证明第一个不等式，我们将凸性的定义反复地应用到和式中的一项上。期望可以通过取有限段的极限来证明。

满足约束条件的另一种策略是投影。同样，我们以前遇到过它们，比如在8.5节中处理渐变剪辑时。在这里，我们确保了梯度的长度以 c 为界通过

TODO:MATH

这就是 g 在半径为 c 的球上的投影。更一般地，(凸)集 X上 的投影定义为

TODO:MATH

因此，它是 X 到 X 的最近点。这听起来有点抽象。[图11.2.4](http://preview.d2l.ai/d2l-en/PR-1102/chapter_optimization/convexity.html#fig-projections)更清楚地说明了这一点。其中有两个凸集，一个圆和一个菱形。集合内的点(黄色)保持不变。集合外的点(黑色)映射到集合内最近的点(红色)。而对于 l2 球，这使方向不变，这不必是一般情况下，可以看到菱形的情况。

凸投影的一个用途是计算稀疏权值向量。在这种情况下，我们将w投影到一个 l1 的球上(后者是上图中钻石的广义版本)。

## 小结

在深度学习的背景下，凸函数的主要目的是激励优化算法，并帮助我们了解它们的细节。下面我们将看到如何相应地推导梯度下降和随机梯度下降。

- 凸集的交集是凸的。 并集不是。
- 凸函数的期望大于期望的凸函数（詹森不等式）。
- 当且仅当其二阶导数在整个过程中仅具有非负特征值时，二次可微函数才是凸函数。
- 可以通过Lagrange函数添加凸约束。 实际上，只需将它们加到目标函数中即可。
- 投影映射到（凸）集中最接近原始点的点。
