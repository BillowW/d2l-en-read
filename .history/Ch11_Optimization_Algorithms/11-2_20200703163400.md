

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-03 16:25:07
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-03 16:34:00
 * @Description:
 * @TODO::
 * @Reference:
-->

凸性在优化算法设计中起着至关重要的作用。这很大程度上是因为在这种环境下分析和测试算法要容易得多。换句话说，如果算法即使在凸集上也表现不佳，我们就不应该希望看到好的结果。此外，尽管深度学习中的优化问题通常是非凸问题，但它们往往在局部极小点附近表现出凸问题的一些性质。这可能导致令人兴奋的新的优化变体，如 [Izmailov et al., 2018](http://preview.d2l.ai/d2l-en/PR-1102/chapter_references/zreferences.html#izmailov-podoprikhin-garipov-ea-2018)。

## 基础



- 凸集的交集是凸的。 并集不是。
- 凸函数的期望大于期望的凸函数（詹森不等式）。
- 当且仅当其二阶导数在整个过程中仅具有非负特征值时，二次可微函数才是凸函数。
- 可以通过Lagrange函数添加凸约束。 实际上，只需将它们加到目标函数中即可。
- 投影映射到（凸）集中最接近原始点的点。
