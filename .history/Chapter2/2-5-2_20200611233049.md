

<!--
 * @version:
 * @Author: steven
 * @Date: 2020-06-11 19:06:46
 * @LastEditors: steven
 * @LastEditTime: 2020-06-11 23:30:49
 * @Description:
-->
I noticed that 2.5.2 does't have PyTorch, so I tried to convert it.

I used ".detach()" and ".clone()" to simulate  ".copy()" in MXNET.

But I'm confused with differences from ".detach()" and ".clone()".

Which one is more like to ".copy()" in MXNET?

```python
from torch.autograd import Variable

x = torch.arange(4.0, requires_grad=True)
# make it as a Variable with a gradient taken.
x =  torch.autograd.Variable(x, requires_grad=True)
y = x * x
y.backward(torch.ones(y.size()))
```

---

AttributeError: 'Tensor' object has no attribute 'copy'

```python
# detach
u = x.detach()

# replace :u = torch.autograd.Variable(u, requires_grad=True)
# make tensor autograd works
u.requires_grad()

v = u * u
v.backward(torch.ones(v.size()))

x.grad == u.grad
```

tensor([True, True, True, True])

```python
# clone
u = x.clone()
u.requires_grad()
v = u * u
v.backward(torch.ones(v.size()))

x.grad == u.grad
```
tensor([True, True, True, True])

---

For more:

1. framework: https://zh.mxnet.io/blog/learn-mxnet-for-pytorch-users

2. torch.autograd.Variable(x, requires_grad=True)：https://www.codetd.com/article/6409254

3. y.backward(torch.ones(y.size()))：https://www.cnblogs.com/wanghui-garcia/p/10629227.html

4. autograd_api: https://pytorch.org/docs/stable/autograd.html
detach or clone:"https://blog.csdn.net/winycg/article/details/100813519

5.深度学习之PyTorch物体检测实战 2.2.1
