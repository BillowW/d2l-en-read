

<!--
 * @version:
 * @Author: steven
 * @Date: 2020-06-11 19:06:46
 * @LastEditors: steven
 * @LastEditTime: 2020-06-11 19:15:43
 * @Description:
-->
My answer to PyTorch vision to 2.5.2
But I'm confuse with the difference from ".detach()" and ".clone()".
Which one is more like to ".copy()" in MXNET?

```python
from torch.autograd import Variable

x = torch.arange(4.0, requires_grad=True)
# make it as a Variable with a gradient taken.
x =  torch.autograd.Variable(x, requires_grad=True)
y = x * x
y.backward(torch.ones(y.size()))
```

---

AttributeError: 'Tensor' object has no attribute 'copy'

```python
# detach
u = x.detach()
u = torch.autograd.Variable(u, requires_grad=True)
v = u * u
v.backward(torch.ones(v.size()))

x.grad == u.grad
```

tensor([True, True, True, True])

```python
# clone
u = x.clone()
u = torch.autograd.Variable(u, requires_grad=True)
v = u * u
v.backward(torch.ones(v.size()))

x.grad == u.grad
```
tensor([True, True, True, True])

---

For more:

framework: https://zh.mxnet.io/blog/learn-mxnet-for-pytorch-users
torch.autograd.Variable(x, requires_grad=True)：https://www.codetd.com/article/6409254
y.backward(torch.ones(y.size()))：https://www.cnblogs.com/wanghui-garcia/p/10629227.html
autograd_api: https://pytorch.org/docs/stable/autograd.html
detach or clone:"https://blog.csdn.net/winycg/article/details/100813519
