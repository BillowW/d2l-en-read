

<!--
 * @version:
 * @Author: steven
 * @Date: 2020-06-11 19:06:46
 * @LastEditors: steven
 * @LastEditTime: 2020-06-11 19:13:30
 * @Description:
-->
My answer to PyTorch vision to 2.5.2
But I'm confuse with the difference from ".detach()" and ".clone()".
Which one is more like to ".copy()" in MXNET?

```python
from torch.autograd import Variable

x = torch.arange(4.0, requires_grad=True)
# make it as a Variable with a gradient taken.
x =  torch.autograd.Variable(x, requires_grad=True)
y = x * x
y.backward(torch.ones(y.size()))
```

---

AttributeError: 'Tensor' object has no attribute 'copy'

```python
# detach
u = x.detach()
u = torch.autograd.Variable(u, requires_grad=True)
v = u * u
v.backward(torch.ones(v.size()))

x.grad == u.grad
```

tensor([True, True, True, True])

```

```python
# clone
u = x.clone()
u = torch.autograd.Variable(u, requires_grad=True)
v = u * u
v.backward(torch.ones(v.size()))

x.grad == u.grad
```
tensor([True, True, True, True])

