

<!--
 * @version:
 * @Author: steven
 * @Date: 2020-06-26 15:26:01
 * @LastEditors: steven
 * @LastEditTime: 2020-06-26 15:53:30
 * @Description:
-->
# 4.9 考虑环境

在前面的章节中，我们通过机器学习的一些实际应用，拟合模型到各种数据集。然而，我们从未停下来去思考数据从何而来，或者我们计划最终会如何处理模型的输出。拥有数据的机器学习开发人员常常急于开发模型，而没有停下来考虑这些基本问题。

许多失败的机器学习使用都可以追溯到这个模式。有时，用测试集的精确度来衡量，模型的表现似乎非常出色，但当数据分布突然发生变化时，就会灾难性地失败。更阴险的是，有时模型的运用本身可能成为扰乱数据分布的催化剂。例如，我们训练了一个模型来预测，对于贷款谁会偿还、谁会违约，结果发现申请人对鞋履的选择与违约风险有关(牛津鞋预示还款，运动鞋预示违约)。在此之后，我们可能倾向于向所有穿牛津鞋的申请者发放贷款，而拒绝所有穿运动鞋的申请者。

在这种情况下，我们从模式识别到决策的轻率跳步（拍脑袋决策），以及我们未能批判性地考虑环境可能会带来灾难性的后果。首先，一旦我们开始根据鞋类产品做决定，客户就会理解并改变他们的行为。用不了多久，所有的申请者都将穿着牛津的衣服，而信用价值却不会有任何相应的提高。花点时间来消化这一点，因为在机器学习的许多应用中都存在类似的问题:通过将基于模型的决策引入环境，我们可能会破坏模型。

尽管我们不可能在一个部分中对这些主题进行完整的处理，但我们的目的是暴露一些常见的问题，并激发批判性思维来及早发现这些情况，减轻损害并负责任地使用机器学习。 有些解决方案很简单（要求“正确的”数据），有些解决方案在技术上很困难（实施强化学习系统），而另一些则要求我们完全走出统计预测的领域，并应对关于算法应用上的道德伦理的哲学之问。

## 4.9.1 分布变化

首先，我们坚持使用被动预测设置，考虑数据分布可能发生变化的各种方式，以及可以采取哪些措施来挽救模型性能。在一个经典的设置中，我们假设我们的训练数据是从某个分布pS(x,y)中采样的，但是我们的测试数据将包含从某个不同分布pT(x,y)中抽取的未标记示例。我们必须面对一个令人清醒的现实。如果没有任何关于pS和pT如何相互关联的假设，学习一个原始分类器是不可能的。

考虑一个二元分类问题，我们希望区分狗和猫。如果分布可以以任意方式移动，那么我们的设置允许这种病理情况，即输入上的分布保持不变:pS(x)=pT(x)，但是标签都翻转了$p S(y |:raw-latex: ' \mathbf{x} ')= 1 - pT(y|x)pT(y|x)。换句话说，如果上帝突然决定，在未来所有的“猫”都是现在的狗，我们以前叫“狗”的是现在的猫——输入p(x)的分布没有任何变化，那么我们就不可能区分这个设置




### 4.9.4. 总结

- 在许多情况下，训练集和测试集并不来自相同的分布。这叫做协变量移位。
- 在相应的假设条件下，可以检测并校正测试时的协变量和标号偏移。如果不能解释这种偏差，在测试时就会出现问题。
- 在某些情况下，环境可能会记住自动化的动作，并以令人惊讶的方式做出反应。当我们构建模型并继续监控实时系统时，我们必须考虑到这种“我们的模型和环境可能会以意想不到的方式纠缠在一起”的可能性。
