

<!--
 * @version:
 * @Author: steven
 * @Date: 2020-06-26 15:26:01
 * @LastEditors: steven
 * @LastEditTime: 2020-06-26 16:15:07
 * @Description:
-->
# 4.9. 考虑环境

在前面的章节中，我们通过机器学习的一些实际应用，拟合模型到各种数据集。然而，我们从未停下来去思考数据从何而来，或者我们计划最终会如何处理模型的输出。拥有数据的机器学习开发人员常常急于开发模型，而没有停下来考虑这些基本问题。

许多失败的机器学习使用都可以追溯到这个模式。有时，用测试集的精确度来衡量，模型的表现似乎非常出色，但当数据分布突然发生变化时，就会灾难性地失败。更阴险的是，有时模型的运用本身可能成为扰乱数据分布的催化剂。例如，我们训练了一个模型来预测，对于贷款谁会偿还、谁会违约，结果发现申请人对鞋履的选择与违约风险有关(牛津鞋预示还款，运动鞋预示违约)。在此之后，我们可能倾向于向所有穿牛津鞋的申请者发放贷款，而拒绝所有穿运动鞋的申请者。

在这种情况下，我们从模式识别到决策的轻率跳步（拍脑袋决策），以及我们未能批判性地考虑环境可能会带来灾难性的后果。首先，一旦我们开始根据鞋类产品做决定，客户就会理解并改变他们的行为。用不了多久，所有的申请者都将穿着牛津的衣服，而信用价值却不会有任何相应的提高。花点时间来消化这一点，因为在机器学习的许多应用中都存在类似的问题:通过将基于模型的决策引入环境，我们可能会破坏模型。

尽管我们不可能在一个部分中对这些主题进行完整的处理，但我们的目的是暴露一些常见的问题，并激发批判性思维来及早发现这些情况，减轻损害并负责任地使用机器学习。 有些解决方案很简单（要求“正确的”数据），有些解决方案在技术上很困难（实施强化学习系统），而另一些则要求我们完全走出统计预测的领域，并应对关于算法应用上的道德伦理的哲学之问。

## 4.9.1. 分布移位

首先，我们坚持使用被动预测设置，考虑数据分布可能发生变化的各种方式，以及可以采取哪些措施来挽救模型性能。在一个经典的设置中，我们假设我们的训练数据是从某个分布pS(x,y)中采样的，但是我们的测试数据将包含从某个不同分布pT(x,y)中抽取的未标记示例。我们必须面对一个令人清醒的现实。如果没有任何关于pS和pT如何相互关联的假设，学习一个原始分类器是不可能的。

考虑一个二元分类问题，我们希望区分狗和猫。如果分布可以以任意方式移动，那么我们的设置允许这种病理情况，即输入上的分布保持不变:pS(x)=pT(x)，但是标签都翻转了$p S(y |:raw-latex: ' \mathbf{x} ')= 1 - pT(y|x)pT(y|x)。换句话说，如果上帝突然决定，在未来所有的“猫”都是现在的狗，我们以前叫“狗”的是现在的猫————输入p(x)的分布没有任何变化，那么我们就不可能区分这个设置与之前一点没变的分布。

### 4.9.1.1. 协变量移位

### 4.9.1.2. 标签移位


标签移位描述了相反的问题。这里，我们假设标签边际P(y)可以改变(导致P(x)的改变)，但类条件分布P(x|y)在区域间保持不变。Label shift是我们认为yy导致xx的合理假设。例如，我们可能希望根据他们的症状(或其他表现)来预测诊断，即使诊断的相对流行率随时间而变化。因为疾病会引起症状，所以标签转换是一个合适的假设。在一些退化的情况下，标签移位和协变

###

在更一般的有记忆的环境中，我们可能会遇到环境试图与我们合作的情况(合作游戏，特别是非零和游戏)，或者环境试图获胜的情况。国际象棋、围棋、西洋双陆棋或星际争霸都属于此类。同样，我们可能想为自动驾驶汽车制造一个好的控制器。其他车辆可能会对自动驾驶汽车的驾驶方式做出非琐碎的反应，比如试图避开它、试图引发事故、试图与它合作等等。


### 4.9.3. 机器学习的公平性、可信性、透明性

最后，重要的是要记住，在部署机器学习系统时，您不仅在优化预测模型，而且还通常提供一种工具，该工具将被用于（部分地或全部地）自动化决策。 这些技术系统可能会影响由其去做决定的人们的生活。
从预测到决策的飞跃，不仅带来了新的技术问题，也带来了一系列必须仔细考虑的道德问题。如果我们部署一个医疗诊断系统，我们需要知道它对哪些人群可能有效，哪些人群可能无效。忽视对亚群体福利可预见的风险可能导致我们实施低劣的护理。
此外，一旦我们考虑决策系统，我们必须后退一步，重新考虑我们如何评价我们的技术。在这种范围变化的其他后果中，我们将发现准确度很少是正确的度量标准。例如，在将预测转化为行动时，我们经常想要考虑各种错误的潜在成本敏感性。如果一种对图像进行误分类的方法被认为是一种种族欺骗，而将其错分类到另一个类别则是无害的，那么我们可能需要相应地调整阈值。

我们也要小心预测系统如何导致反馈循环。例如，考虑预测性警务系统，它将巡逻人员分配到犯罪率高的地区。很容易看出一种令人担忧的模式会出现:犯罪较多的社区会有更多的巡逻人员。因此，在这些社区中会发现更多的犯罪行为，从而输入可供未来迭代使用的训练数据。如果接触到更多的积极因素，该模型预测这些社区会出现更多的犯罪。在接下来的迭代中，更新后的模型将会以同样的邻域为目标





这可能会导致研究人员所说的失控反馈循环。此外，我们首先要注意我们是否处理了正确的问题。预测算法现在在调解信息传播方面发挥着巨大的作用。个人遭遇的新闻应该由他们喜欢的Facebook页面决定吗?这些只是你在机器学习的职业生涯中可能遇到的许多紧迫的伦理困境中的一小部分。


### 4.9.4. 总结

- 在许多情况下，训练集和测试集并不来自相同的分布。这叫做协变量移位。
- 在相应的假设条件下，可以检测并校正测试时的协变量和标号偏移。如果不能解释这种偏差，在测试时就会出现问题。
- 在某些情况下，环境可能会记住自动化的动作，并以令人惊讶的方式做出反应。当我们构建模型并继续监控实时系统时，我们必须考虑到这种“我们的模型和环境可能会以意想不到的方式纠缠在一起”的可能性

### 4.9.5. 练习

1. 当我们改变搜索引擎的行为时会发生什么?用户可能会做什么?那广告商呢?
2. 实现一个协变量移位检测器。提示:构建一个分类器。
3. 实现一个协变量移位校正器。
4. 如果训练集和测试集非常不同，会出现什么问题?样本权重会发生什么变化
