

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-03 19:33:12
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-03 19:37:01
 * @Description:
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/PR-1102/chapter_attention-mechanisms/seq2seq-attention.html
-->

#
在本节中，我们将注意机制添加到[9.7节](http://preview.d2l.ai/d2l-en/PR-1102/chapter_recurrent-modern/seq2seq.html#sec-seq2seq)中介绍的序列到序列(seq2seq)模型中，以显式地用权重聚合状态。图10.2.1显示了在时间步长t进行编码和解码的模型架构。在这里，注意层的记忆由编码器在每个时间步长所看到的编码器输出的所有信息组成。在解码期间，使用前一个时间步长t1t1的解码器输出作为查询。注意模型的输出被视为上下文信息，并与d连接

现在我们可以用注意力模型来测试seq2seq。为了与9.7节中的模型保持一致，我们对vocab大小、嵌入大小、num隐藏和num层使用相同的超参数。结果，我们得到相同的解码器输出形状，但状态结构改变。

在解码的每个时间步长，我们使用解码器的最后一层RNN的输出作为关注层的查询。然后将注意力模型的输出与输入嵌入向量连接到RNN层中。虽然RNN层隐藏状态也包含了解码器的历史信息，但是attention输出显式地选择了基于enc有效len的编码器输出，使得attention输出挂起了其他不相关的信息。

让我们实现`Seq2SeqAttentionDecoder`，看看它与9.7.2节中的seq2seq中的解码器有什么不同。

与第9.7.4节类似，我们使用相同的训练超参数和相同的训练损失来尝试一个玩具模型。从结果中我们可以看出，由于训练数据集中的序列相对较短，额外的注意层并没有带来显著的改善。由于编码器和解码器注意层的计算开销，该模型比不注意时的seq2seq模型要慢得多。

最后，我们预测了几个示例。

TODO:CODE

- 带有注意力的seq2seq模型向不带有注意力的模型添加了额外的注意力层。
- 带有注意力模型的seq2seq解码器从编码器中传递三项内容:编码器所有时间步长的输出、编码器最终时间步长的隐藏状态和编码器有效长度。

## 练习

1. 使用相同的参数比较Seq2SeqAttentionDecoder和Seq2seqDecoder，并检查它们的损耗。
1. 你能想到Seq2SeqAttentionDecoder比Seq2seqDecoder更好的用例吗
