

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-03 19:47:41
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-03 19:54:48
 * @Description:
 * @TODO::
 * @Reference:
-->

# 注意力机制

在[9.7节](http://preview.d2l.ai/d2l-en/PR-1102/chapter_recurrent-modern/seq2seq.html#sec-seq2seq)中，我们将源序列输入信息编码为周期性单元状态，然后将其传递给解码器生成目标序列。目标序列中的标记可能与源序列中的一个或多个标记密切相关，而不是整个源序列。例如，在翻译Hello world时。你好，《世界报》。例如，Bonjour映射到Hello, monde映射到world。在seq2seq模型中，解码器可以隐式地从编码器传递的状态中选择相应的信息。

TODO:CODE

正如我们在上面看到的，点积注意力只是将查询和键相乘，并希望从中得到它们的相似性。然而，查询和键可能不是同一维的。为了解决这个问题，我们可以求助于多层感知器。

直观上，你可以把 Wkk+Wqq 想象为将特征维中的键和值连接起来，并将它们输入到一个隐含层大小为 h 、输出层大小为 1 的单层感知器中。在这个隐层中，激活函数为 tanh ，不存在偏置。现在让我们注意实现多层感知器。

为了测试上面的`MLPAttention`类，我们使用与上一个玩具示例相同的输入。正如我们在下面看到的，尽管`MLPAttention`包含了一个额外的MLP模型，但我们得到的输出与`DotProductAttention`相同。


## 小结

- 注意层显式地选择相关信息。
- 注意层的内存由键-值对组成，因此它的输出接近键与查询相似的值。
- 两种常用的注意模型是点积注意和多层感知器注意。

## 练习

1. 点积关注和多层感知器关注分别有什么优点和缺点?
