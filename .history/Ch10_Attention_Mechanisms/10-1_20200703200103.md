

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-03 19:47:41
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-03 20:01:03
 * @Description:
 * @TODO::
 * @Reference:
-->

# 注意力机制

在[9.7节](http://preview.d2l.ai/d2l-en/PR-1102/chapter_recurrent-modern/seq2seq.html#sec-seq2seq)中，我们将源序列输入信息编码为周期性单元状态，然后将其传递给解码器生成目标序列。目标序列中的标记可能与源序列中的一个或多个标记密切相关，而不是整个源序列。例如，当翻译Hello world时。Bonjour le monde。例如，Bonjour映射到Hello, monde映射到world。在seq2seq模型中，解码器可以隐式地从编码器传递的状态中选择相应的信息。

注意是一种通用的合并方法，其输入上存在偏差对齐。 注意机制的核心组件是注意层，为简单起见称为关注。 注意层的输入称为查询。 对于查询，注意力返回基于内存的输出-记忆层中编码的一组键-值对。 更具体地说，假设存储器包含n个键值对（k1，v1），…，（kn，vn），其中ki∈Rdk，vi∈Rdv。 给定查询q∈Rdq，注意层返回形状与值相同的输出o∈Rdv。


让我们在一个玩具样例中测试类DotProductAttention。首先，创建两个批处理，其中每个批处理有一个查询和10个键-值对。通过valid len参数，我们指定将检查第一个批处理的前22个键值对和第二个批处理的66个键值对。因此，即使这两个批具有相同的查询和键-值对，我们也会获得不同的输出。

TODO:CODE

正如我们在上面看到的，点积注意力只是将查询和键相乘，并希望从中得到它们的相似性。然而，查询和键可能不是同一维的。为了解决这个问题，我们可以求助于多层感知器注意。

## 多层感知器

在多层感知器注意中，我们通过可学习的权重参数将查询和键都投影到Rh中。 假设可学习的权重为Wk∈Rh×dk，Wq∈Rh×dq和v∈Rh。 然后得分函数定义为

TODO:MATH

直观上，你可以把 Wkk+Wqq 想象为将特征维中的键和值连接起来，并将它们输入到一个隐含层大小为 h 、输出层大小为 1 的单层感知器中。在这个隐层中，激活函数为 tanh ，不存在偏置。现在让我们实现多层感知器注意。

为了测试上面的`MLPAttention`类，我们使用与上一个玩具示例相同的输入。正如我们在下面看到的，尽管`MLPAttention`包含了一个额外的MLP模型，但我们得到的输出与`DotProductAttention`相同。


## 小结

- 注意层显式地选择相关信息。
- 注意层的内存由键-值对组成，因此它的输出接近键与查询相似的值。
- 两种常用的注意模型是点积注意和多层感知器注意。

## 练习

1. 点积关注和多层感知器关注分别有什么优点和缺点?
