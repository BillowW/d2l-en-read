

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-03 19:47:41
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-03 19:52:00
 * @Description:
 * @TODO::
 * @Reference:
-->

直观上，你可以把 Wkk+Wqq 想象为将特征维中的键和值连接起来，并将它们输入到一个隐含层大小为 h 、输出层大小为 1 的单层感知器中。在这个隐层中，激活函数为 tanh ，不存在偏置。现在让我们注意实现多层感知器。

为了测试上面的`MLPAttention`类，我们使用与上一个玩具示例相同的输入。正如我们在下面看到的，尽管`MLPAttention`包含了一个额外的MLP模型，但我们得到的输出与`DotProductAttention`相同。


## 小结

- 注意层显式地选择相关信息。
- 注意层的内存由键-值对组成，因此它的输出接近键与查询相似的值。
- 两种常用的注意模型是点积注意和多层感知器注意。

## 练习
1. 点积关注和多层感知器关注分别有什么优点和缺点?
