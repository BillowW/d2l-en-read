

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-05 17:06:57
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-05 17:08:56
 * @Description:
 * @TODO::
 * @Reference:
-->

# 几何和线性代数运算

在第2.3节中，我们学习了线性代数的基础知识，并了解了如何使用线性代数表示转换数据的常见操作。线性代数是支撑我们深度学习和更广泛的机器学习的许多工作的关键数学支柱之一。尽管2.3部分包含了足够的机制来传达现代深度学习模型的机制，但还有更多的内容。在这一节中，我们将更深入地探讨，重点介绍线性代数运算的几何解释，并介绍一些基本概念，包括特征矢量。

## 矢量的几何学

首先，我们需要讨论向量的两种常见的几何解释，作为空间中的点或方向。基本上，向量是一个数字列表，如下面的 Python 列表。

数学家们经常把它写成一个列或行向量，也就是说，要么写成

## 总结

* 向量可以在几何上解释为空间中的点或方向。
* 点积将角度的概念定义为任意高维空间。
* 超平面是线和平面的高维概括。 它们可用于定义决策平面，这些决策平面通常用作分类任务的最后一步。
* 矩阵乘法可以在几何上解释为基础坐标的均匀变形。 它们代表了一种非常有限但数学上干净的向量转换方法。
* 线性相关性是一种判断向量集合何时处于比我们期望的空间空间小的方法的方法（例如，您有3个向量生活在二维空间中）。 矩阵的等级是线性独立的其列的最大子集的大小。
* 定义矩阵的逆矩阵后，矩阵求逆可以让我们找到另一个矩阵，该矩阵可以消除第一个矩阵的作用。 矩阵求逆理论上很有用，但由于数值不稳定，在实践中需要谨慎。
* 行列式使我们能够衡量矩阵扩展或缩小空间的程度。 非零行列式表示一个可逆的（非奇异）矩阵，零值行列式表示该矩阵是不可逆的（奇异）。
* 张量收缩和爱因斯坦求和为表达机器学习中看到的许多计算提供了一种简洁明了的符号。
