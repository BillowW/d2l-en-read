

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-07 13:17:16
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-07 13:17:41
 * @Description:
 * @TODO::
 * @Reference:
-->


现在，我们可以确切地看到我们所希望的！ 用原理特征值对矩阵进行归一化后，我们看到随机数据不会像以前那样爆炸，而是最终平衡为一个特定值。 能够从第一性原理开始做这些事情将是一件很高兴的事，结果发现，如果我们深入研究它的数学运算，我们可以看到，一个独立均值为零，方差为一个高斯的大型随机矩阵的最大特征值 由于一个被称为循环法则的引人入胜的事实[Ginibre，1965]，平均条目平均约为n-√，在本例中为5-√≈2.2。 如[Pennington等人，2017]和后续工作中所讨论的，随机矩阵的特征值（和一个称为奇异值的相关对象）之间的关系已证明与神经网络的正确初始化有着深厚的联系。

特征向量是由一个矩阵拉伸而不改变方向的向量。特征值是特征向量被应用矩阵拉伸的量。矩阵的特征分解可以使许多运算简化为对特征值的运算。格尔什戈林圆定理可以给出矩阵特征值的近似值。迭代矩阵幂的性质主要取决于最大特征值的大小。这种理解在神经网络初始化理论中有很多应用。

特征值和特征向量是什么
下面矩阵的特征值和特征向量是什么，这个例子和之前的例子相比有什么奇怪的地方
不计算特征值，下面矩阵的最小特征值是否可能小于0。50.5 ?注意:这个问题可以心算。
