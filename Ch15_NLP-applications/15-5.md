

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-31 19:56:39
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-31 20:22:26
 * @Description:MT
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_natural-language-processing-applications/natural-language-inference-attention.html
-->

# 自然语言推理: 注意力的运用

在15.4节中，我们介绍了自然语言推理任务和 SNLI 数据集。鉴于许多模型是基于复杂和深层的架构，Parikh 等人提出解决自然语言推理与注意机制，并称之为“可分解注意模型”[ Parikh 等人，2016]。这导致了一个没有循环层或卷积层的模型，在 SNLI 数据集上用更少的参数取得了最好的结果。在本节中，我们将描述并实现这种基于注意力的自然语言推理方法(使用 MLPs) ，如图15.5.1所示。

图15.5.1本部分将预先训练的 GloVe 提供给基于注意力和 mlp 的自然语言推理架构。

## 模型

比起在前提和假设中保持单词的顺序更简单的是，我们可以将一个文本序列中的单词与另一个文本序列中的每个单词对齐，反之亦然，然后比较和聚合这些信息来预测前提和假设之间的逻辑关系。类似于机器翻译中源语句和目标语句之间的词语对齐，前提和假设之间的词语对齐可以通过注意机制巧妙地完成。

图15.5.2使用注意机制的自然语言推理。

图15.5.2描绘了使用注意机制的自然语言推理方法。在高层次上，它包括三个共同培训的步骤: 参加，比较和聚集。我们将在下面一步一步地说明它们。

TODO:CODE

## 参与

第一步是将一个文本序列中的单词与另一个序列中的每个单词对齐。假设前提是“我确实需要睡眠” ，而假设是“我累了”。由于语义相似性，我们可能希望将假设中的“ i”与前提中的“ i”对齐，并将假设中的“ tired”与前提中的“ sleep”对齐。同样，我们可能希望将前提中的“ i”与假设中的“ i”对齐，并将前提中的“需要”和“睡眠”与假设中的“累”对齐。注意，这样的对齐方式是柔性的，使用加权平均数，其中理想的大权重与要对齐的单词相关联。为了便于演示，图15.5.2显示了这种硬的方式排列。

现在我们使用注意力机制更详细地描述软对齐。表示 a = (a1，... ，a m) a = (a1，... ，am)和 b = (b1，... ，b n) b = (b1，... ，bn)前提和假设，其中 a i，bj ∈ Rd (i = 1，... ，m，j = 1，... ，n)是 d 维嵌入向量。对于软比对，我们计算注意力的权重 ij ∈ r eij ∈ r

## 训练和评估模型

现在我们将训练和评估SNLI数据集上定义的可分解注意力模型。我们从读取数据集开始。

### 阅读数据集

我们使用15.4节中定义的函数下载和读取SNLI数据集。批大小和序列长度分别设置为256和50。

TODO:CODE

### 创建模型

我们使用预先训练好的100100维手套嵌入来表示输入标记。因此，我们将:eqref:eq_nli_e中的向量aiai和bjbj的维数预定义为100100。函数ff in(15.5.1)和gg in:eqref:eq_nli_v_ab的输出维数设置为200200。然后创建一个模型实例，初始化其参数，并加载嵌入的手套来初始化输入令牌的向量。

TODO:CODE

### 训练和评估模型

其中函数 f 是在下面的 mlp 函数中定义的多层感知机。F 的输出维度由 mlp 的 num _ hiddens 参数指定。

与12.5节中的split_batch函数接受单个输入(如文本序列(或图像))不同，我们定义了一个split_batch_multi_input函数来接受多个输入(如在小批量中接受前提和假设)。

TODO:CODE

现在我们可以在SNLI数据集上训练和评估模型。

TODO:CODE

### 使用模型

最后，定义预测函数，输出一对前提与假设之间的逻辑关系。

TODO:CODE

我们可以使用训练过的模型来获得一个句子样本对的自然语言推理结果。

TODO:CODE

## 小结

* 可分解注意模型包括预测前提与假设之间的逻辑关系的三个步骤:注意、比较和聚集。
* 通过注意机制，我们可以将一个文本序列中的单词与另一个文本序列中的每个单词对齐，反之亦然。使用加权平均，这种对齐是软的，在理想情况下，较大的权重与要对齐的单词相关联。
* 在计算注意力权重时，分解技巧可以得到一个比二次复杂度更理想的线性复杂度。
* 我们可以使用预先训练好的字嵌入作为下游自然语言处理任务(如自然语言推理)的输入表示。

## 练习

1. 用其他超参数组合训练模型。你能在测试集上获得更好的精确度吗?
1. 自然语言推理中可分解注意模型的主要缺点是什么?
1. 假设我们想要得到任意一对句子的语义相似程度(例如，00到11之间的连续值)。我们如何收集和标签数据集?你能设计一个有注意机制的模型吗?
