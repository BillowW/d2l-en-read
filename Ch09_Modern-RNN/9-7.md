

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-29 21:45:51
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-29 21:59:50
 * @Description:MT half
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_recurrent-modern/seq2seq.html
-->



TODO：CODE

试几个例子：

TODO：CODE

## 小结

* 序列到序列(seq2seq)模型基于编码器-解码器架构，从序列输入生成序列输出。
* 我们为编码器和解码器使用多个LSTM层。


## 练习

1. 除了神经机器翻译，你还能想到seq2seq的其他用例吗?
1. 如果本节示例中的输入序列较长，该怎么办?
1. 如果我们在损失函数中不使用序列运算，会发生什么?
