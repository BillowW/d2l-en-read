

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-19 20:00:08
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-19 20:05:33
 * @Description:translate by machine
 * @TODO::
 * @Reference:https://d2l.ai/chapter_recurrent-neural-networks/bptt.html
-->

到目前为止，我们反复提到了诸如爆炸梯度、消失梯度、截断反螺旋以及需要分离计算图表之类的东西。例如，在上一节中，我们对序列调用了 s.detach ()。为了能够快速构建模型并了解其工作原理，所有这些都没有得到充分的解释。在本节中，我们将更深入地研究序列模型的反向传播的细节，以及这种数学运算的原理和方式。关于随机化和反向传播的更详细的讨论，请参阅[ Tallec & Ollivier，2017]的论文。

当我们第一次实现回归神经网络时，我们遇到了梯度爆炸的一些影响(第8.5节)。特别是，如果你解决了问题集中的问题，你会发现梯度裁剪对于确保正确的收敛是至关重要的。为了更好地理解这个问题，本节将回顾如何为序列模型计算梯度。请注意，它的工作方式在概念上没有什么新的东西。毕竟，我们仍然只是应用链式规则来计算梯度。尽管如此，还是值得再次回顾一下反向传播(4.7节)。

在递归神经网络中的正向传播是相对简单的。时间反向传播算法是反向传播算法在递归神经网络中的一个具体应用。它要求我们一次一个步骤地展开递归神经网络，以获得模型变量和参数之间的依赖关系。然后，基于链式规则，应用反向传播算法计算和存储梯度。由于序列可能相当长，依赖关系可能相当长。例如，对于1000个字符的序列，第一个符号可能对位置1000处的符号有重大影响。这在计算上是不可行的(它需要太长的时间和太多的内存) ，而且在我们达到这个难以捉摸的梯度之前，它需要超过1000个矩阵向量乘积。这是一个充满计算和统计不确定性的过程。下面我们将阐明在实践中会发生什么以及如何解决这个问题。

我们从一个 RNN 如何工作的简化模型开始。这个模型忽略了关于隐藏状态的细节以及如何更新的细节。这些细节对于分析来说是无关紧要的，只会使符号变得杂乱无章，但是会使它看起来更令人生畏。在这个简化的模型中，我们用 h t t t 表示隐藏状态，x t t t 表示输入，o t 表示时间步 t t 的输出。另外，w h wh 和 w o 分别表示隐状态和输出层的权重。因此，每个时间步骤的隐藏状态和输出可以解释为

因此，我们有一个值链{ ... ，(h t-1，x t-1，o t-1) ，(h t，x t，o t) ，... }{ ... ，(ht-1，xt-1，ot-1) ，(ht，xt，ot) ，... } ，它们通过递归计算相互依赖。正向传播是相当简单的。我们所需要做的就是一次循环完成(xt，ht，ot)(xt，ht，ot)三元组。然后用目标函数作为评价指标，计算出期望目标的输出值与期望目标的偏差

对于反向传播来说，问题就有点棘手了，尤其是当我们根据目标函数 l 的参数来计算梯度时。具体来说，根据链式法则,

导数的第一部分和第二部分很容易计算。在第三部分，我们需要计算出参数对系统的影响，这也是影响系统性能的关键因素。



* 时间反向传播只是反向传播应用于具有隐藏状态的序列模型。
* 为了计算方便和数值稳定性，需要截断。
* 矩阵的高次幂可能导致特征值发散和消失。 这以爆炸或消失的梯度形式表现出来。
* 为了高效计算，中间值被缓存。

1. 假设我们有一个对称矩阵M∈Rn×nM∈Rn×n，特征值是λiλi。 在不失一般性的前提下，假定它们以升序λi≤λi+1λi≤λi+ 1排序。 证明MkMk具有特征值λkiλik。
1. 证明对于一个随机向量x∈Rnx∈Rn，MkxMkx很有可能与MM的最大特征向量vnvn高度对齐。 将此语句形式化。
1. 以上结果对于递归神经网络中的梯度意味着什么？
1. 除了梯度削波，您还能想到其他方法来应对递归神经网络中的梯度爆炸吗？
