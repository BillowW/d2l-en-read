

<!--
 * @version:
 * @Author: steven
 * @Date: 2020-06-26 15:26:01
 * @LastEditors: steven
 * @LastEditTime: 2020-06-26 17:50:17
 * @Description:
-->
# 4.9. 考虑环境

在前面的章节中，我们通过机器学习的一些实际应用，拟合模型到各种数据集。然而，我们从未停下来去思考数据从何而来，或者我们计划最终会如何处理模型的输出。拥有数据的机器学习开发人员常常急于开发模型，而没有停下来考虑这些基本问题。

许多失败的机器学习使用都可以追溯到这个模式。有时，用测试集的精确度来衡量，模型的表现似乎非常出色，但当数据分布突然发生变化时，就会灾难性地失败。更阴险的是，有时模型的运用本身可能成为扰乱数据分布的催化剂。例如，我们训练了一个模型来预测，对于贷款谁会偿还、谁会违约，结果发现申请人对鞋履的选择与违约风险有关(牛津鞋预示还款，运动鞋预示违约)。在此之后，我们可能倾向于向所有穿牛津鞋的申请者发放贷款，而拒绝所有穿运动鞋的申请者。

在这种情况下，我们从模式识别到决策的轻率跳步（拍脑袋决策），以及我们未能批判性地考虑环境可能会带来灾难性的后果。首先，一旦我们开始根据鞋类产品做决定，客户就会理解并改变他们的行为。用不了多久，所有的申请者都将穿着牛津的衣服，而信用价值却不会有任何相应的提高。花点时间来消化这一点，因为在机器学习的许多应用中都存在类似的问题:通过将基于模型的决策引入环境，我们可能会破坏模型。

尽管我们不可能在一个部分中对这些主题进行完整的处理，但我们的目的是暴露一些常见的问题，并激发批判性思维来及早发现这些情况，减轻损害并负责任地使用机器学习。有些解决方案很简单（要求“正确的”数据），有些解决方案在技术上很困难（实施强化学习系统），而另一些则要求我们完全走出统计预测的领域，并应对关于算法应用上的道德伦理的哲学之问。

## 4.9.1. 分布移位

首先，我们坚持使用被动预测设置，考虑数据分布可能发生变化的各种方式，以及可以采取哪些措施来挽救模型性能。在一个经典的设置中，我们假设我们的训练数据是从某个分布pS(x,y)中采样的，但是我们的测试数据将包含从某个不同分布pT(x,y)中抽取的未标记示例。我们必须面对一个令人清醒的现实。如果没有任何关于pS和pT如何相互关联的假设，学习一个原始分类器是不可能的。

考虑一个二元分类问题，我们希望区分狗和猫。如果分布可以以任意方式移动，那么我们的设置允许这种病理情况，即输入上的分布保持不变:pS(x)=pT(x)，但是标签都翻转了$p S(y |:raw-latex: ' \mathbf{x} ')= 1 - pT(y|x)pT(y|x)。换句话说，如果上帝突然决定，在未来所有的“猫”都是现在的狗，我们以前叫“狗”的是现在的猫————输入p(x)的分布没有任何变化，那么我们就不可能区分这个设置与之前一点没变的分布。

### 4.9.1.1. 协变量移位

在分布转移的类别中，协变量转移可能是研究最广泛的。在这里，我们假设输入的分布可能随时间而改变，标记函数，即。，条件分布P(y|x)不变。统计学家称之为协变量转移，因为问题的出现是由于协变量(特征)分布的转移。虽然我们有时可以在不援引因果关系的情况下对分布转移进行推理，但我们注意到，在我们认为x导致y的情况下，协变量转移是一种自然假设。

想想区分猫和狗的挑战吧。我们的训练数据可能由以下类型的图像组成

### 4.9.1.2. 标签移位


标签移位描述了相反的问题。这里，我们假设标签边际P(y)可以改变(导致P(x)的改变)，但类条件分布P(x|y)在区域间保持不变。Label shift是我们认为yy导致xx的合理假设。例如，我们可能希望根据他们的症状(或其他表现)来预测诊断，即使诊断的相对流行率随时间而变化。因为疾病会引起症状，所以标签转换是一个合适的假设。在一些退化的情况下，标签移位和协变

### 4.9.1.4. 例子

在深入研究形式主义和算法之前，我们可以讨论一些具体的情况，其中协变量或概念转移可能不明显。

#### 4.9.1.4.1.医学诊断

假设你想设计一个算法来检测癌症。你从健康和生病的人那里收集数据，然后训练你的算法。它工作得很好，给你很高的准确度，你就会得出结论，你已经准备好在医学诊断领域取得成功了。*没有那么快。*

产生训练数据的分布和你将在野外遇到的分布可能有很大的不同。这发生在几年前我们工作的一家初创公司。他们正在为一种主要影响老年人的疾病开发一种血液测试，并希望利用从患者身上采集的血液样本进行研究。然而，从健康男性身上获取血液样本要比从已经在系统中的病人身上获取血液样本困难得多。作为补偿，这家初创公司在一所大学校园里征集学生捐献血液，作为开发测试时的健康对照。然后他们问我们是否可以帮助他们建立一个分类器来检测这种疾病。

正如我们向他们解释的那样，确实很容易以近乎完美的准确性区分健康和生病的人群。然而，这是因为测试对象在年龄、激素水平、体力活动、饮食、饮酒以及许多其他与疾病无关的因素方面存在差异。这种情况不太可能出现在真正的病人身上。由于他们的采样程序，我们可能会遇到极端的协变量位移。此外，这种情况不大可能通过传统方法加以纠正。总之，他们浪费了一大笔钱。

#### 4.9.1.4.2. 自动驾驶汽车

假设一家公司想利用机器学习来开发自动驾驶汽车。其中一个关键部件是路边探测器。因为获得真正的注释数据是昂贵的，所以他们有一个(聪明但有问题的)想法，即使用来自游戏渲染引擎的合成数据作为额外的训练数据。这对于从渲染引擎中提取的测试数据非常有效。唉，在一辆真正的汽车里，这简直是一场灾难。结果，路边被渲染成一个非常简单的纹理。更重要的是，所有的路边都被“渲染”成同样的纹理。

当美军第一次试图在森林中侦察坦克时，也发生了类似的事情。他们在没有坦克的情况下拍摄了森林的航拍照片，然后把坦克开进森林，又拍了一组照片。这个分类器似乎工作得很好。不幸的是，它仅仅学会了如何区分有阴影的树和没有阴影的树。第一组照片拍摄于清晨，第二组拍摄于中午。

#### 4.9.1.4.3. 非平稳分布

当分布变化缓慢且模型更新不充分时，会出现更为微妙的情况。下面是一些典型的例子:
- 我们训练了一个计算广告模型，但却没有频繁地更新它(例如，我们忘记了一款名为iPad的默默无闻的新设备刚刚上市)。
- 我们建立了一个垃圾邮件过滤器。它能很好地检测到我们目前所见过的所有垃圾邮件。但是，垃圾邮件发送者会想出一些新的信息，看起来和我们以前见过的任何信息都不一样。
- 我们建立了一个产品推荐系统。它在整个冬天都能工作，但在圣诞节过后很长一段时间，它仍在推荐圣诞老人帽子。

#### 4.9.1.4.4. 更多轶事

- 我们造了一个人脸检测器。它在所有基准测试中都运行良好。不幸的是，它在测试数据上失败了。违规的例子是脸部填充整个图像的特写镜头(训练集中没有这样的数据)。
- 我们为美国市场建立一个网络搜索引擎，并想在英国部署它。
- 我们通过编译一个大数据集来训练一个图像分类器，其中大集合类中的每一个都在数据集中相等地表示，比如1000个类别，每个类别用1000幅图像表示。然后我们在现实世界中部署该系统，照片的实际标签分布显然是不统一的。

简而言之，在很多情况下，训练分布和测试分布p(x,y)p(x,y)是不同的。在某些情况下，我们很幸运，尽管有协变量、标签或概念的改变，模型仍然有效。在其他情况下，我们可以通过有原则的策略来更好地应对这种转变。本节的其余部分将增加相当多的技术性内容。没有耐心的读者可以继续阅读下一节，因为这些材料并不是后续概念的先决条件。



## 4.9.2. 学习问题的分类

有了如何处理p(x)和p(y|x)变化的知识，我们现在可以考虑机器学习问题公式的其他方面。

- **批量学习：**这里我们可以访问训练数据和标签{(x1,y1)， (xn,yn)}{(x1,y1)， (xn,yn)}，我们用它来训练一个网络f(x,w)f(x,w)。随后，我们部署这个网络从相同的分布中获取新的数据(x,y)(x,y)。这是我们在这里讨论的任何问题的默认假设。例如，我们可以根据大量的猫和狗的图片训练猫探测器。一旦我们训练了它，我们就把它作为智能猫门计算机视觉系统的一部分运送出去，这个系统只允许猫进入。这是然后安装在客户的家里，永远不会再更新(除非极端情况
- **在线学习：**现在假设数据(xi,yi)(xi,yi)一次到达一个样本。更具体地说，假设我们首先观察xixi，然后我们需要提出一个估计f(xi,w)f(xi,w)，只有当我们这样做了，我们观察yiyi，根据我们的决定，我们得到一个奖励(或招致一个损失)。许多真正的问题都属于这一类。例如，我们需要预测明天的股票价格，这样我们就可以根据预测进行交易，在一天结束的时候，我们就能知道我们的预测是否允许我们获利。换句话说，我们有一个循环
- 土匪。它们是上述问题的一个特例。在大多数学习问题中，我们有一个连续的参数化函数ff，我们想要学习它的参数(例如，一个深度网络)，而在土匪问题中，我们只有有限数量的武器，我们可以拉(例如，一个深度网络)。我们可以采取的行动数量有限)。对于这个更简单的问题，在最优性方面可以得到更强的理论保证，这并不奇怪。我们列出它主要是因为这个问题经常(令人困惑地)被当作一个独特的学习设置来对待。
- **控制(和非对抗的强化学习)：**在很多情况下，环境会记得我们做了什么。不一定是对抗的方式，但它会记住，反应将取决于之前发生了什么。例如，咖啡锅炉控制器会根据之前是否在加热锅炉来观察不同的温度。其中PID(proportional integral derivative 比例积分导数)控制算法是一种流行的选择。同样地，用户在新闻站点上的行为也取决于我们之前向他展示的内容(例如，他只会阅读大多数新闻一次)。许多这样的算法形成了一个环境模型，在其中它们的行为是这样的
- **增强学习：**在更一般的有记忆的环境中，我们可能会遇到环境试图与我们合作的情况(合作游戏，特别是非零和游戏)，或者环境试图获胜的情况。国际象棋、围棋、西洋双陆棋或星际争霸都属于此类。同样，我们可能想为自动驾驶汽车制造一个好的控制器。其他车辆可能会对自动驾驶汽车的驾驶方式做出非琐碎的反应，比如试图避开它、试图引发事故、试图与它合作等等。


### 4.9.3. 机器学习的公平性、可信性、透明性

最后，重要的是要记住，在部署机器学习系统时，您不仅在优化预测模型，而且还通常提供一种工具，该工具将被用于（部分地或全部地）自动化决策。这些技术系统可能会影响由其去做决定的人们的生活。
从预测到决策的飞跃，不仅带来了新的技术问题，也带来了一系列必须仔细考虑的道德问题。如果我们部署一个医疗诊断系统，我们需要知道它对哪些人群可能有效，哪些人群可能无效。忽视对亚群体福利可预见的风险可能导致我们实施低劣的护理。
此外，一旦我们考虑决策系统，我们必须后退一步，重新考虑我们如何评价我们的技术。在这种范围变化的其他后果中，我们将发现准确度很少是正确的度量标准。例如，在将预测转化为行动时，我们经常想要考虑各种错误的潜在成本敏感性。如果一种对图像进行误分类的方法被认为是一种种族欺骗，而将其错分类到另一个类别则是无害的，那么我们可能需要相应地调整阈值。

我们也要小心预测系统如何导致反馈循环。例如，考虑预测性警务系统，它将巡逻人员分配到犯罪率高的地区。很容易看出一种令人担忧的模式会出现:犯罪较多的社区会有更多的巡逻人员。因此，在这些社区中会发现更多的犯罪行为，从而输入可供未来迭代使用的训练数据。如果接触到更多的积极因素，该模型预测这些社区会出现更多的犯罪。在接下来的迭代中，更新后的模型将会以同样的邻域为目标





这可能会导致研究人员所说的失控反馈循环。此外，我们首先要注意我们是否处理了正确的问题。预测算法现在在调解信息传播方面发挥着巨大的作用。个人遭遇的新闻应该由他们喜欢的Facebook页面决定吗?这些只是你在机器学习的职业生涯中可能遇到的许多紧迫的伦理困境中的一小部分。


### 4.9.4. 总结

- 在许多情况下，训练集和测试集并不来自相同的分布。这叫做协变量移位。
- 在相应的假设条件下，可以检测并校正测试时的协变量和标号偏移。如果不能解释这种偏差，在测试时就会出现问题。
- 在某些情况下，环境可能会记住自动化的动作，并以令人惊讶的方式做出反应。当我们构建模型并继续监控实时系统时，我们必须考虑到这种“我们的模型和环境可能会以意想不到的方式纠缠在一起”的可能性

### 4.9.5. 练习

1. 当我们改变搜索引擎的行为时会发生什么?用户可能会做什么?那广告商呢?
2. 实现一个协变量移位检测器。提示:构建一个分类器。
3. 实现一个协变量移位校正器。
4. 如果训练集和测试集非常不同，会出现什么问题?样本权重会发生什么变化
